{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred2, y_dev):\n",
    "    print(f\"accuracy: {metrics.accuracy_score(y_pred2, y_dev)}\")\n",
    "    print(f\"f1 score: {metrics.f1_score(y_pred2, y_dev)}\")\n",
    "    print(f\"AUROC: {metrics.roc_auc_score(y_pred2, y_dev)}\")\n",
    "    print(f\"Recall: {metrics.recall_score(y_pred2, y_dev)}\") \n",
    "    print(f\"Precision: {metrics.precision_score(y_pred2, y_dev)}\")\n",
    "    \n",
    "df= pd.read_pickle(\"./catalog.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df[[\"train\" in val for val in df[\"sets\"]]].drop([\"file\", \"sets\"], axis=1)\n",
    "df_test= df[[\"test_seen\" in val for val in df[\"sets\"]]].drop([\"file\", \"sets\"], axis=1)\n",
    "df_val= df[[\"dev_seen\" in val for val in df[\"sets\"]]].drop([\"file\", \"sets\"], axis=1)\n",
    "df_test2= df[[\"test_unseen\" in val for val in df[\"sets\"]]].drop([\"file\", \"sets\"], axis=1)\n",
    "df_val2= df[[\"dev_unseen\" in val for val in df[\"sets\"]]].drop([\"file\", \"sets\"], axis=1)\n",
    "\n",
    "X_test, y_test= df_test.drop([\"label\"], axis=1) , df_test[\"label\"]\n",
    "X_val, y_val= df_val.drop([\"label\"], axis=1) , df_val[\"label\"]\n",
    "X_test2, y_test2= df_test2.drop([\"label\"], axis=1) , df_test2[\"label\"]\n",
    "X_val2, y_val2= df_val2.drop([\"label\"], axis=1) , df_val2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = df_train[df_train['label'] == 0]\n",
    "class_1_data = df_train[df_train['label'] == 1]\n",
    "oversampled_class_1 = class_1_data.sample(n=2462, random_state=42) #there are 3019 label 1 rows\n",
    "oversampled_df = pd.concat([class_0_data, oversampled_class_1, class_1_data])\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "undersampled_class_0 = class_0_data.sample(n=3019, random_state=42) #there are 3019 label 1 rows\n",
    "undersampled_df = pd.concat([undersampled_class_0, class_1_data])\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "smote = SMOTE(random_state=43)\n",
    "X_train, y_train= df_train.drop([\"label\"], axis=1) , df_train[\"label\"]\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train_oversampled, y_train_oversampled = oversampled_df.drop([\"label\"], axis=1) , oversampled_df[\"label\"]\n",
    "X_train_undersampled, y_train_undersampled = undersampled_df.drop([\"label\"], axis=1) , undersampled_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "Dev Seen: \n",
      "accuracy: 0.506\n",
      "f1 score: 0.2108626198083067\n",
      "AUROC: 0.5015842281288505\n",
      "Recall: 0.13360323886639677\n",
      "Precision: 0.5\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.5\n",
      "f1 score: 0.21630094043887146\n",
      "AUROC: 0.49295718287314927\n",
      "Recall: 0.14081632653061224\n",
      "Precision: 0.46621621621621623\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.5944444444444444\n",
      "f1 score: 0.1978021978021978\n",
      "AUROC: 0.4998529411764706\n",
      "Recall: 0.135\n",
      "Precision: 0.3698630136986301\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.5905\n",
      "f1 score: 0.2222222222222222\n",
      "AUROC: 0.5035999999999999\n",
      "Recall: 0.156\n",
      "Precision: 0.38613861386138615\n",
      "\n",
      "\n",
      "\n",
      "UNDERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.48\n",
      "f1 score: 0.49612403100775193\n",
      "AUROC: 0.4804531852586772\n",
      "Recall: 0.5182186234817814\n",
      "Precision: 0.4758364312267658\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.475\n",
      "f1 score: 0.4956772334293948\n",
      "AUROC: 0.47601040416166474\n",
      "Recall: 0.5265306122448979\n",
      "Precision: 0.46823956442831216\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.47962962962962963\n",
      "f1 score: 0.42061855670103093\n",
      "AUROC: 0.4858823529411765\n",
      "Recall: 0.51\n",
      "Precision: 0.35789473684210527\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.49\n",
      "f1 score: 0.45686900958466453\n",
      "AUROC: 0.5064\n",
      "Recall: 0.572\n",
      "Precision: 0.3803191489361702\n",
      "\n",
      "\n",
      "\n",
      "OVERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.494\n",
      "f1 score: 0.44880174291938996\n",
      "AUROC: 0.4930870045286522\n",
      "Recall: 0.41700404858299595\n",
      "Precision: 0.4858490566037736\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.485\n",
      "f1 score: 0.46961894953656025\n",
      "AUROC: 0.4846138455382153\n",
      "Recall: 0.46530612244897956\n",
      "Precision: 0.47401247401247404\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.512962962962963\n",
      "f1 score: 0.38694638694638694\n",
      "AUROC: 0.4927941176470588\n",
      "Recall: 0.415\n",
      "Precision: 0.3624454148471616\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.4885\n",
      "f1 score: 0.41509433962264153\n",
      "AUROC: 0.4876\n",
      "Recall: 0.484\n",
      "Precision: 0.3633633633633634\n",
      "\n",
      "\n",
      "\n",
      "SMOTE\n",
      "Dev Seen: \n",
      "accuracy: 0.49\n",
      "f1 score: 0.41379310344827586\n",
      "AUROC: 0.4885103454897506\n",
      "Recall: 0.3643724696356275\n",
      "Precision: 0.4787234042553192\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.496\n",
      "f1 score: 0.43875278396436523\n",
      "AUROC: 0.49415766306522607\n",
      "Recall: 0.4020408163265306\n",
      "Precision: 0.48284313725490197\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.5203703703703704\n",
      "f1 score: 0.3541147132169576\n",
      "AUROC: 0.4863235294117647\n",
      "Recall: 0.355\n",
      "Precision: 0.35323383084577115\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.505\n",
      "f1 score: 0.3888888888888889\n",
      "AUROC: 0.488\n",
      "Recall: 0.42\n",
      "Precision: 0.3620689655172414\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_set= [X_train, X_train_undersampled, X_train_oversampled, X_train_smote]\n",
    "y_set= [y_train, y_train_undersampled, y_train_oversampled, y_train_smote]\n",
    "strv=[\"ORIGINAL\", \"UNDERSAMPLED\", \"OVERSAMPLED\", \"SMOTE\"]\n",
    "\n",
    "for i in range(4):\n",
    "    print(strv[i])\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_set[i], y_set[i])\n",
    "\n",
    "    print(\"Dev Seen: \")\n",
    "    y_val_pred = rf_classifier.predict(X_val)\n",
    "    print_metrics(y_val, y_val_pred)\n",
    "    print(\"\\nTest Seen: \")\n",
    "    y_test_pred = rf_classifier.predict(X_test)\n",
    "    print_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nDev Unseen: \")\n",
    "    y_val_pred2 = rf_classifier.predict(X_val2)\n",
    "    print_metrics(y_val2, y_val_pred2)\n",
    "    print(\"\\nTest Unseen: \")\n",
    "    y_test_pred2 = rf_classifier.predict(X_test2)\n",
    "    print_metrics(y_test2, y_test_pred2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "Dev Seen: \n",
      "accuracy: 0.51\n",
      "f1 score: 0.02390438247011952\n",
      "AUROC: 0.5040965899089469\n",
      "Recall: 0.012145748987854251\n",
      "Precision: 0.75\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.509\n",
      "f1 score: 0.00808080808080808\n",
      "AUROC: 0.4990996398559424\n",
      "Recall: 0.004081632653061225\n",
      "Precision: 0.4\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.6277777777777778\n",
      "f1 score: 0.028985507246376812\n",
      "AUROC: 0.5016176470588235\n",
      "Recall: 0.015\n",
      "Precision: 0.42857142857142855\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.6255\n",
      "f1 score: 0.007947019867549669\n",
      "AUROC: 0.5012\n",
      "Recall: 0.004\n",
      "Precision: 0.6\n",
      "\n",
      "\n",
      "\n",
      "UNDERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.512\n",
      "f1 score: 0.47639484978540775\n",
      "AUROC: 0.5112576210974381\n",
      "Recall: 0.4493927125506073\n",
      "Precision: 0.5068493150684932\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.549\n",
      "f1 score: 0.5247629083245522\n",
      "AUROC: 0.5481992797118848\n",
      "Recall: 0.5081632653061224\n",
      "Precision: 0.5424836601307189\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.5148148148148148\n",
      "f1 score: 0.4099099099099099\n",
      "AUROC: 0.5025000000000001\n",
      "Recall: 0.455\n",
      "Precision: 0.3729508196721312\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.5615\n",
      "f1 score: 0.4649176327028676\n",
      "AUROC: 0.5508000000000001\n",
      "Recall: 0.508\n",
      "Precision: 0.42857142857142855\n",
      "\n",
      "\n",
      "\n",
      "OVERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.512\n",
      "f1 score: 0.42990654205607476\n",
      "AUROC: 0.5103454897505241\n",
      "Recall: 0.3724696356275304\n",
      "Precision: 0.5082872928176796\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.533\n",
      "f1 score: 0.461361014994233\n",
      "AUROC: 0.5305522208883553\n",
      "Recall: 0.40816326530612246\n",
      "Precision: 0.5305039787798409\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.5351851851851852\n",
      "f1 score: 0.3771712158808933\n",
      "AUROC: 0.503235294117647\n",
      "Recall: 0.38\n",
      "Precision: 0.37438423645320196\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.571\n",
      "f1 score: 0.4226110363391655\n",
      "AUROC: 0.5405333333333333\n",
      "Recall: 0.4186666666666667\n",
      "Precision: 0.4266304347826087\n",
      "\n",
      "\n",
      "\n",
      "SMOTE\n",
      "Dev Seen: \n",
      "accuracy: 0.506\n",
      "f1 score: 0.4642082429501085\n",
      "AUROC: 0.5051367396905154\n",
      "Recall: 0.4331983805668016\n",
      "Precision: 0.5\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.545\n",
      "f1 score: 0.4972375690607735\n",
      "AUROC: 0.5433173269307723\n",
      "Recall: 0.45918367346938777\n",
      "Precision: 0.5421686746987951\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.5333333333333333\n",
      "f1 score: 0.4166666666666667\n",
      "AUROC: 0.5161764705882352\n",
      "Recall: 0.45\n",
      "Precision: 0.3879310344827586\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.5565\n",
      "f1 score: 0.44318895166352795\n",
      "AUROC: 0.5393333333333333\n",
      "Recall: 0.4706666666666667\n",
      "Precision: 0.41874258600237246\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_set= [X_train, X_train_undersampled, X_train_oversampled, X_train_smote]\n",
    "y_set= [y_train, y_train_undersampled, y_train_oversampled, y_train_smote]\n",
    "strv=[\"ORIGINAL\", \"UNDERSAMPLED\", \"OVERSAMPLED\", \"SMOTE\"]\n",
    "\n",
    "for i in range(4):\n",
    "    print(strv[i])\n",
    "    svm_classifier = SVC()\n",
    "    svm_classifier.fit(X_set[i], y_set[i])\n",
    "\n",
    "    print(\"Dev Seen: \")\n",
    "    y_val_pred = svm_classifier.predict(X_val)\n",
    "    print_metrics(y_val, y_val_pred)\n",
    "    print(\"\\nTest Seen: \")\n",
    "    y_test_pred = svm_classifier.predict(X_test)\n",
    "    print_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nDev Unseen: \")\n",
    "    y_val_pred2 = svm_classifier.predict(X_val2)\n",
    "    print_metrics(y_val2, y_val_pred2)\n",
    "    print(\"\\nTest Unseen: \")\n",
    "    y_test_pred2 = svm_classifier.predict(X_test2)\n",
    "    print_metrics(y_test2, y_test_pred2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "Dev Seen: \n",
      "accuracy: 0.508\n",
      "f1 score: 0.0390625\n",
      "AUROC: 0.5022163191499576\n",
      "Recall: 0.020242914979757085\n",
      "Precision: 0.5555555555555556\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.514\n",
      "f1 score: 0.03571428571428571\n",
      "AUROC: 0.504281712685074\n",
      "Recall: 0.018367346938775512\n",
      "Precision: 0.6428571428571429\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.6296296296296297\n",
      "f1 score: 0.047619047619047616\n",
      "AUROC: 0.5051470588235295\n",
      "Recall: 0.025\n",
      "Precision: 0.5\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.6235\n",
      "f1 score: 0.03088803088803089\n",
      "AUROC: 0.502\n",
      "Recall: 0.016\n",
      "Precision: 0.4444444444444444\n",
      "\n",
      "\n",
      "\n",
      "UNDERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.5\n",
      "f1 score: 0.5454545454545454\n",
      "AUROC: 0.50127218319438\n",
      "Recall: 0.6072874493927125\n",
      "Precision: 0.49504950495049505\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.512\n",
      "f1 score: 0.5555555555555556\n",
      "AUROC: 0.5141656662665066\n",
      "Recall: 0.6224489795918368\n",
      "Precision: 0.5016447368421053\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.4685185185185185\n",
      "f1 score: 0.4595103578154426\n",
      "AUROC: 0.49764705882352933\n",
      "Recall: 0.61\n",
      "Precision: 0.3685800604229607\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.491\n",
      "f1 score: 0.48272357723577236\n",
      "AUROC: 0.5194666666666666\n",
      "Recall: 0.6333333333333333\n",
      "Precision: 0.3899835796387521\n",
      "\n",
      "\n",
      "\n",
      "OVERSAMPLED\n",
      "Dev Seen: \n",
      "accuracy: 0.488\n",
      "f1 score: 0.5492957746478874\n",
      "AUROC: 0.4897025171624714\n",
      "Recall: 0.631578947368421\n",
      "Precision: 0.48598130841121495\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.526\n",
      "f1 score: 0.5782918149466192\n",
      "AUROC: 0.5286914765906363\n",
      "Recall: 0.6632653061224489\n",
      "Precision: 0.5126182965299685\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.4648148148148148\n",
      "f1 score: 0.4716636197440585\n",
      "AUROC: 0.5019117647058824\n",
      "Recall: 0.645\n",
      "Precision: 0.37175792507204614\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.4855\n",
      "f1 score: 0.4928536224741252\n",
      "AUROC: 0.5217333333333334\n",
      "Recall: 0.6666666666666666\n",
      "Precision: 0.39093041438623927\n",
      "\n",
      "\n",
      "\n",
      "SMOTE\n",
      "Dev Seen: \n",
      "accuracy: 0.5\n",
      "f1 score: 0.5598591549295775\n",
      "AUROC: 0.5017042454113392\n",
      "Recall: 0.6437246963562753\n",
      "Precision: 0.4953271028037383\n",
      "\n",
      "Test Seen: \n",
      "accuracy: 0.522\n",
      "f1 score: 0.5709156193895871\n",
      "AUROC: 0.5244897959183674\n",
      "Recall: 0.6489795918367347\n",
      "Precision: 0.5096153846153846\n",
      "\n",
      "Dev Unseen: \n",
      "accuracy: 0.4703703703703704\n",
      "f1 score: 0.48\n",
      "AUROC: 0.5094117647058825\n",
      "Recall: 0.66\n",
      "Precision: 0.37714285714285717\n",
      "\n",
      "Test Unseen: \n",
      "accuracy: 0.4895\n",
      "f1 score: 0.48924462231115556\n",
      "AUROC: 0.522\n",
      "Recall: 0.652\n",
      "Precision: 0.3915132105684548\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_set= [X_train, X_train_undersampled, X_train_oversampled, X_train_smote]\n",
    "y_set= [y_train, y_train_undersampled, y_train_oversampled, y_train_smote]\n",
    "strv=[\"ORIGINAL\", \"UNDERSAMPLED\", \"OVERSAMPLED\", \"SMOTE\"]\n",
    "\n",
    "for i in range(4):\n",
    "    print(strv[i])\n",
    "    lr_classifier = LogisticRegression(max_iter=1000)\n",
    "    lr_classifier.fit(X_set[i], y_set[i])\n",
    "\n",
    "    print(\"Dev Seen: \")\n",
    "    y_val_pred = lr_classifier.predict(X_val)\n",
    "    print_metrics(y_val, y_val_pred)\n",
    "    print(\"\\nTest Seen: \")\n",
    "    y_test_pred = lr_classifier.predict(X_test)\n",
    "    print_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nDev Unseen: \")\n",
    "    y_val_pred2 = lr_classifier.predict(X_val2)\n",
    "    print_metrics(y_val2, y_val_pred2)\n",
    "    print(\"\\nTest Unseen: \")\n",
    "    y_test_pred2 = lr_classifier.predict(X_test2)\n",
    "    print_metrics(y_test2, y_test_pred2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "1. Precision= True Positives / (True Positives + False Positives). This means that having a fairly high precision implies that the false positives are lower. This means, chances of my model saying that something non-offensive (class 0) is actually offensive (class 1) is lower. \n",
    "2. Recall= True Positives / (True Positives + False Negatives). This means that having a fairly low recall means greater number of false negatives, ie if my meme is offensive in reality, but my model ends up predicting it as non-offensive\n",
    "3. In summary, when my model is predicting something to be offensive, it is correct, but the model ends up missing a lot of the ofensive memes too. This is consistent with our hypothesis- that using only text or only image is not going to be helpful, because we lose out on critical information. When we are doing classification based on an image catalogue, we end up ignoring half the information- the caption (and in many cases, captions do change the meaning). At the same time, we are also ignoring some information from the image too, as we have identified what is in the image, but not what they are doing - ie, the actions. \n",
    "4. However, when we apply dataset balancing techniques such as SMOTE, Undersampling and oversampling, the performance improves significantly in terms of recall and f1 score. Conventionally used classification models such as SVMs, Logistic Regression and Random Forests are sensitive to unbalanced data.\n",
    "5. Despite this, we still notice that our model does not perform very well, and is still below baselines and human performance and this happens because we are ignoring the text component of our memes completely. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
