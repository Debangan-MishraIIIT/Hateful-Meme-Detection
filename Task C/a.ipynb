{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred2, y_dev):\n",
    "    print(f\"accuracy: {metrics.accuracy_score(y_pred2, y_dev)}\")\n",
    "    print(f\"f1 score: {metrics.f1_score(y_pred2, y_dev)}\")\n",
    "    print(f\"AUROC: {metrics.roc_auc_score(y_pred2, y_dev)}\")\n",
    "    print(f\"Recall: {metrics.recall_score(y_pred2, y_dev)}\") \n",
    "    print(f\"Precision: {metrics.precision_score(y_pred2, y_dev)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>sets</th>\n",
       "      <th>preprocessed text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>hate count</th>\n",
       "      <th>text encoding</th>\n",
       "      <th>cleaned encoding</th>\n",
       "      <th>image encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>berserk 2016 is a good adaptation you're kidd...</td>\n",
       "      <td>71094.png</td>\n",
       "      <td>[test_unseen]</td>\n",
       "      <td>berserk good adaptation you kid right</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[-0.09067752957344055, 0.2159278392791748, 0.0...</td>\n",
       "      <td>[-0.2386399209499359, 0.26847022771835327, 0.1...</td>\n",
       "      <td>[0.08480989187955856, -0.07862813770771027, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my life goal? make somebody this fucking trig...</td>\n",
       "      <td>91724.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>life goal make somebody this fucking trigger</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>[0.10644109547138214, 0.03420567512512207, 0.0...</td>\n",
       "      <td>[-0.038524020463228226, -0.08633303642272949, ...</td>\n",
       "      <td>[-0.23801393806934357, -0.2769679129123688, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" i don't wanna, just get it, get it, get it, ...</td>\n",
       "      <td>64280.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>not wanna get get get get that shit hard chanc...</td>\n",
       "      <td>-0.163889</td>\n",
       "      <td>0.480556</td>\n",
       "      <td>0.080</td>\n",
       "      <td>[0.33546730875968933, -0.33385685086250305, 0....</td>\n",
       "      <td>[0.2787761092185974, -0.23599806427955627, -0....</td>\n",
       "      <td>[-0.0899612158536911, -0.3343321681022644, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text       file  \\\n",
       "0      0   berserk 2016 is a good adaptation you're kidd...  71094.png   \n",
       "1      0   my life goal? make somebody this fucking trig...  91724.png   \n",
       "2      0  \" i don't wanna, just get it, get it, get it, ...  64280.png   \n",
       "\n",
       "            sets                                  preprocessed text  polarity  \\\n",
       "0  [test_unseen]              berserk good adaptation you kid right  0.492857   \n",
       "1        [train]       life goal make somebody this fucking trigger -0.600000   \n",
       "2        [train]  not wanna get get get get that shit hard chanc... -0.163889   \n",
       "\n",
       "   subjectivity  hate count  \\\n",
       "0      0.567857       0.000   \n",
       "1      0.800000       0.125   \n",
       "2      0.480556       0.080   \n",
       "\n",
       "                                       text encoding  \\\n",
       "0  [-0.09067752957344055, 0.2159278392791748, 0.0...   \n",
       "1  [0.10644109547138214, 0.03420567512512207, 0.0...   \n",
       "2  [0.33546730875968933, -0.33385685086250305, 0....   \n",
       "\n",
       "                                    cleaned encoding  \\\n",
       "0  [-0.2386399209499359, 0.26847022771835327, 0.1...   \n",
       "1  [-0.038524020463228226, -0.08633303642272949, ...   \n",
       "2  [0.2787761092185974, -0.23599806427955627, -0....   \n",
       "\n",
       "                                      image encoding  \n",
       "0  [0.08480989187955856, -0.07862813770771027, 0....  \n",
       "1  [-0.23801393806934357, -0.2769679129123688, -0...  \n",
       "2  [-0.0899612158536911, -0.3343321681022644, -0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_pickle(\"../commons/clip_embeddings.pkl\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sets</th>\n",
       "      <th>text encoding</th>\n",
       "      <th>image encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[test_unseen]</td>\n",
       "      <td>[-0.09067752957344055, 0.2159278392791748, 0.0...</td>\n",
       "      <td>[0.08480989187955856, -0.07862813770771027, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[train]</td>\n",
       "      <td>[0.10644109547138214, 0.03420567512512207, 0.0...</td>\n",
       "      <td>[-0.23801393806934357, -0.2769679129123688, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[train]</td>\n",
       "      <td>[0.33546730875968933, -0.33385685086250305, 0....</td>\n",
       "      <td>[-0.0899612158536911, -0.3343321681022644, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           sets                                      text encoding  \\\n",
       "0      0  [test_unseen]  [-0.09067752957344055, 0.2159278392791748, 0.0...   \n",
       "1      0        [train]  [0.10644109547138214, 0.03420567512512207, 0.0...   \n",
       "2      0        [train]  [0.33546730875968933, -0.33385685086250305, 0....   \n",
       "\n",
       "                                      image encoding  \n",
       "0  [0.08480989187955856, -0.07862813770771027, 0....  \n",
       "1  [-0.23801393806934357, -0.2769679129123688, -0...  \n",
       "2  [-0.0899612158536911, -0.3343321681022644, -0....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2= df.drop([\"text\", \"file\", \"preprocessed text\", \"polarity\", \"subjectivity\", \"hate count\", \"cleaned encoding\"], axis=1)\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= df2[[\"train\" in val for val in df2[\"sets\"]]].drop([\"sets\"], axis=1)\n",
    "test_set= df2[[\"test_seen\" in val for val in df2[\"sets\"]]].drop([\"sets\"], axis=1)\n",
    "val_set= df2[[\"dev_seen\" in val for val in df2[\"sets\"]]].drop([\"sets\"], axis=1)\n",
    "\n",
    "test_set_unseen= df2[[\"test_unseen\" in val for val in df2[\"sets\"]]].drop([\"sets\"], axis=1)\n",
    "val_set_unseen= df2[[\"dev_unseen\" in val for val in df2[\"sets\"]]].drop([\"sets\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling is done to reduce imbalance. It is however found that the model performs similar without the undersampling\n",
    "#The state dicts of original model are in original folder while the undersampled one is in same directory\n",
    "#Every new rerun stores the state dicts in the rerun folder\n",
    "\n",
    "class_0_data = train_set[train_set['label'] == 0]\n",
    "class_1_data = train_set[train_set['label'] == 1]\n",
    "undersampled_class_0 = class_0_data.sample(n=3019, random_state=42) #there are 3019 label 1 rows\n",
    "undersampled_df = pd.concat([undersampled_class_0, class_1_data])\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_set = undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_encoding = torch.tensor(self.data.iloc[index]['text encoding'])\n",
    "        image_encoding = torch.tensor(self.data.iloc[index]['image encoding'])\n",
    "        label = torch.tensor(self.data.iloc[index]['label'])\n",
    "        \n",
    "        return text_encoding, image_encoding, label\n",
    "\n",
    "train_dataset = CustomDataset(train_set)\n",
    "val_dataset = CustomDataset(val_set)\n",
    "test_dataset = CustomDataset(test_set)\n",
    "\n",
    "val_dataset_unseen = CustomDataset(val_set_unseen)\n",
    "test_dataset_unseen = CustomDataset(test_set_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model= nn.Sequential(\n",
    "            nn.Linear(512, 216),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(216, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Outputter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Outputter, self).__init__()\n",
    "        self.model= nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(16, 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x, y):\n",
    "        val= torch.mul(x, y)\n",
    "        return self.model(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.0005\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "text_encoder= Encoder()\n",
    "image_encoder= Encoder()\n",
    "output_model= Outputter()\n",
    "\n",
    "loss_fn= nn.CrossEntropyLoss()\n",
    "# optim= torch.optim.SGD(chain(text_encoder.parameters(), image_encoder.parameters(), output_model.parameters()), lr=lr)\n",
    "\n",
    "optim_text= torch.optim.Adam(text_encoder.parameters(), lr=lr)\n",
    "optim_img= torch.optim.Adam(image_encoder.parameters(), lr=lr)\n",
    "optim_total= torch.optim.SGD(output_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_state_text = None\n",
    "best_model_state_img = None\n",
    "best_model_state_total = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,\n",
      "          Train Acc: 49.98%,\n",
      "          Val Acc: 50.60%,\n",
      "          Test Acc: 51.00%\n",
      "\n",
      "Epoch 2/20,\n",
      "          Train Acc: 53.71%,\n",
      "          Val Acc: 60.00%,\n",
      "          Test Acc: 63.20%\n",
      "\n",
      "Epoch 3/20,\n",
      "          Train Acc: 60.93%,\n",
      "          Val Acc: 60.20%,\n",
      "          Test Acc: 63.70%\n",
      "\n",
      "Epoch 4/20,\n",
      "          Train Acc: 63.65%,\n",
      "          Val Acc: 61.60%,\n",
      "          Test Acc: 65.90%\n",
      "\n",
      "Epoch 5/20,\n",
      "          Train Acc: 65.14%,\n",
      "          Val Acc: 62.60%,\n",
      "          Test Acc: 65.90%\n",
      "\n",
      "Epoch 6/20,\n",
      "          Train Acc: 66.35%,\n",
      "          Val Acc: 63.20%,\n",
      "          Test Acc: 64.90%\n",
      "\n",
      "Epoch 7/20,\n",
      "          Train Acc: 67.11%,\n",
      "          Val Acc: 63.20%,\n",
      "          Test Acc: 65.90%\n",
      "\n",
      "Epoch 8/20,\n",
      "          Train Acc: 68.77%,\n",
      "          Val Acc: 64.40%,\n",
      "          Test Acc: 65.70%\n",
      "\n",
      "Epoch 9/20,\n",
      "          Train Acc: 69.12%,\n",
      "          Val Acc: 65.20%,\n",
      "          Test Acc: 66.40%\n",
      "\n",
      "Epoch 10/20,\n",
      "          Train Acc: 70.53%,\n",
      "          Val Acc: 64.00%,\n",
      "          Test Acc: 66.10%\n",
      "\n",
      "Epoch 11/20,\n",
      "          Train Acc: 69.73%,\n",
      "          Val Acc: 65.60%,\n",
      "          Test Acc: 66.40%\n",
      "\n",
      "Epoch 12/20,\n",
      "          Train Acc: 70.86%,\n",
      "          Val Acc: 63.20%,\n",
      "          Test Acc: 67.20%\n",
      "\n",
      "Epoch 13/20,\n",
      "          Train Acc: 70.64%,\n",
      "          Val Acc: 63.60%,\n",
      "          Test Acc: 66.50%\n",
      "\n",
      "Epoch 14/20,\n",
      "          Train Acc: 72.27%,\n",
      "          Val Acc: 63.80%,\n",
      "          Test Acc: 66.10%\n",
      "\n",
      "Epoch 15/20,\n",
      "          Train Acc: 72.48%,\n",
      "          Val Acc: 62.60%,\n",
      "          Test Acc: 68.10%\n",
      "\n",
      "Epoch 16/20,\n",
      "          Train Acc: 71.89%,\n",
      "          Val Acc: 63.00%,\n",
      "          Test Acc: 66.50%\n",
      "\n",
      "Epoch 17/20,\n",
      "          Train Acc: 72.60%,\n",
      "          Val Acc: 63.80%,\n",
      "          Test Acc: 68.70%\n",
      "\n",
      "Epoch 18/20,\n",
      "          Train Acc: 72.95%,\n",
      "          Val Acc: 61.40%,\n",
      "          Test Acc: 68.10%\n",
      "\n",
      "Epoch 19/20,\n",
      "          Train Acc: 72.86%,\n",
      "          Val Acc: 63.00%,\n",
      "          Test Acc: 66.70%\n",
      "\n",
      "Epoch 20/20,\n",
      "          Train Acc: 74.07%,\n",
      "          Val Acc: 63.40%,\n",
      "          Test Acc: 68.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    text_encoder.train()\n",
    "    image_encoder.train()\n",
    "    output_model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (text_data, image_data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        optim_text.zero_grad()\n",
    "        optim_img.zero_grad()\n",
    "        optim_total.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        text_output = text_encoder(text_data)\n",
    "        image_output = image_encoder(image_data)\n",
    "        outputs = output_model(text_output, image_output)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optim_text.step()\n",
    "        optim_img.step()\n",
    "        optim_total.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    train_accuracy = 100. * correct / total\n",
    "    \n",
    "    #Validation\n",
    "    text_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    output_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (text_data, image_data, targets) in enumerate(val_loader):\n",
    "            # Forward pass\n",
    "            text_output = text_encoder(text_data)\n",
    "            image_output = image_encoder(image_data)\n",
    "            outputs = output_model(text_output, image_output)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    val_accuracy = 100. * correct / total\n",
    "    \n",
    "    # Testing\n",
    "    text_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    output_model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (text_data, image_data, targets) in enumerate(test_loader):\n",
    "            # Forward pass\n",
    "            text_output = text_encoder(text_data)\n",
    "            image_output = image_encoder(image_data)\n",
    "            outputs = output_model(text_output, image_output)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Update statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    if((test_loss+val_loss)/2<=best_val_loss):\n",
    "        best_model_state_total= output_model.state_dict()\n",
    "        best_model_state_img= image_encoder.state_dict()\n",
    "        best_model_state_text= text_encoder.state_dict()\n",
    "    \n",
    "    test_accuracy = 100. * correct / total\n",
    "    \n",
    "    print(f\"\"\"Epoch {epoch + 1}/{num_epochs},\n",
    "          Train Acc: {train_accuracy:.2f}%,\n",
    "          Val Acc: {val_accuracy:.2f}%,\n",
    "          Test Acc: {test_accuracy:.2f}%\\n\"\"\")\n",
    "\n",
    "torch.save(best_model_state_total, './reruns/best_model_total.pth')\n",
    "torch.save(best_model_state_img, './reruns/best_model_img.pth')\n",
    "torch.save(best_model_state_text, './reruns/best_model_text.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_dataset):\n",
    "    text_enc= torch.stack([val[0] for val in test_dataset])\n",
    "    image_enc= torch.stack([val[1] for val in test_dataset])\n",
    "    y_test= torch.stack([val[2] for val in test_dataset])\n",
    "\n",
    "    text_output = text_encoder(text_enc)\n",
    "    image_output = image_encoder(image_enc)\n",
    "    outputs = output_model(text_output, image_output)\n",
    "    _, predicted = outputs.max(1)\n",
    "    print_metrics(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outputter(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=16, out_features=2, bias=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): ReLU()\n",
       "    (10): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.load_state_dict(torch.load('best_model_text.pth'))\n",
    "image_encoder.load_state_dict(torch.load('best_model_img.pth'))\n",
    "output_model.load_state_dict(torch.load('best_model_total.pth'))\n",
    "text_encoder.eval()\n",
    "image_encoder.eval()\n",
    "output_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.68\n",
      "f1 score: 0.6404494382022472\n",
      "AUROC: 0.6854166666666666\n",
      "Recall: 0.7125\n",
      "Precision: 0.5816326530612245\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.634\n",
      "f1 score: 0.5924276169265034\n",
      "AUROC: 0.6379327530068444\n",
      "Recall: 0.6584158415841584\n",
      "Precision: 0.5384615384615384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.676\n",
      "f1 score: 0.5462184873949579\n",
      "AUROC: 0.6514532821014017\n",
      "Recall: 0.5752212389380531\n",
      "Precision: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_dataset_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.662962962962963\n",
      "f1 score: 0.5404040404040404\n",
      "AUROC: 0.6377847650688182\n",
      "Recall: 0.5459183673469388\n",
      "Precision: 0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(val_dataset_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs better than all the models that were trained only on text or only on image. This is because this model takes into account both modalities- text and image, and takes their dot product. We are able to train and optimise the three constituent models individually and this allows for further flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
