{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "nlp.add_pipe('spacytextblob')\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'i', 'me', 'my', 'myself', 'it', 'its', 'itself', 'what', 'which', 'who', 'whom', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n",
    "    'own', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "\n",
    "hate_speech_words = [\n",
    "    'nigger', 'nigga', 'coon', 'spic', 'chink', 'kike', 'gook', 'wop', 'towelhead', 'slope',\n",
    "    'kafir', 'infidel', 'heathen', 'idolater', 'heretic', 'blasphemer',\n",
    "    'bitch', 'slut', 'whore', 'cunt', 'feminazi', 'tranny', 'dyke', 'faggot', 'fag', 'pussy', 'mansplainer',\n",
    "    'faggot', 'fag', 'dyke', 'homo', 'queer', 'sissy', 'tranny', 'butch',\n",
    "    'tranny', 'shemale', 'it', 'transvestite', 'gender bender', 'freak',\n",
    "    'retard', 'spaz', 'cripple', 'invalid', 'lame', 'imbecile', 'moron', 'idiot',\n",
    "    'fucktard', 'asshat', 'retard', 'douchebag', 'assclown', 'twat', 'twatwaffle', 'dickwad'\n",
    "]\n",
    "\n",
    "swear_words = [\n",
    "    'fuck', 'fucking', 'fucked', 'fuckwit', 'fucktard', 'fuckface', 'fuckhead', 'motherfucker', 'shit', 'shitty', 'shite', 'shitting', \n",
    "    'shithead', 'shitstorm', 'ass', 'asshole', 'asshat', 'asswipe', 'dumbass', 'bitch', 'bitchy', 'son of a bitch',\n",
    "    'bastard', 'damn', 'damned', 'dick', 'dickhead', 'dickwad', 'pussy', 'pussies', 'cock', 'cocksucker', 'cockhead',\n",
    "    'cunt', 'cunty', 'cuntface', 'twat', 'twatwaffle', 'wanker', 'wank', 'wankstain', 'bollocks', 'douchebag', 'douche', 'douchecanoe',\n",
    "    'arse', 'arsehole', 'arsehat', 'arsewipe', 'shitfuck', 'fuckshit', 'clusterfuck', 'fucknugget', 'fuckery',\n",
    "    'cockwomble', 'cockgobbler', 'cockjockey', 'dickcheese', 'dicknose', 'shitlord', 'assclown', 'assjacket'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "      rets=[]\n",
    "      polarity=[]\n",
    "      subjectivity=[]\n",
    "      hate_count=[]\n",
    "      for sent in sentences:\n",
    "            sent = sent.replace(\"n't\", \" not\")\n",
    "            sent = sent.lower().strip()\n",
    "            temp_arr= sent.split(\" \")\n",
    "            count=0\n",
    "            for word in temp_arr:\n",
    "                  if(word in hate_speech_words or word in swear_words):\n",
    "                        count+=1\n",
    "            hate_count.append(count/len(temp_arr))\n",
    "            doc = nlp(sent)\n",
    "            arr = [token.lemma_ for token in doc if (token.lemma_.lower() not in STOP_WORDS) and not token.is_punct and not token.like_num \n",
    "                  and not token.is_currency and not token.is_digit]\n",
    "            arr= \" \".join(arr)\n",
    "            rets.append(arr)\n",
    "            polarity.append(doc._.blob.polarity)\n",
    "            subjectivity.append(doc._.blob.subjectivity)\n",
    "      return rets, polarity, subjectivity, hate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>description</th>\n",
       "      <th>sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>berserk 2016 is a good adaptation you're kidd...</td>\n",
       "      <td>71094.png</td>\n",
       "      <td>woman with a monkey mask and a fake monkey</td>\n",
       "      <td>[test_unseen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my life goal? make somebody this fucking trig...</td>\n",
       "      <td>91724.png</td>\n",
       "      <td>woman holding a cigarette in her hand</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" i don't wanna, just get it, get it, get it, ...</td>\n",
       "      <td>64280.png</td>\n",
       "      <td>man wearing a hat and a tie</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"1st day of 4th grade sandy hook elementary sc...</td>\n",
       "      <td>67082.png</td>\n",
       "      <td>group of children standing in front of a schoo...</td>\n",
       "      <td>[dev_seen, dev_unseen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"a blow job a day will keep his side chicks aw...</td>\n",
       "      <td>46380.png</td>\n",
       "      <td>woman with a black top and a blue background</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text       file  \\\n",
       "0      0   berserk 2016 is a good adaptation you're kidd...  71094.png   \n",
       "1      0   my life goal? make somebody this fucking trig...  91724.png   \n",
       "2      0  \" i don't wanna, just get it, get it, get it, ...  64280.png   \n",
       "3      0  \"1st day of 4th grade sandy hook elementary sc...  67082.png   \n",
       "4      0  \"a blow job a day will keep his side chicks aw...  46380.png   \n",
       "\n",
       "                                         description                    sets  \n",
       "0         woman with a monkey mask and a fake monkey           [test_unseen]  \n",
       "1              woman holding a cigarette in her hand                 [train]  \n",
       "2                        man wearing a hat and a tie                 [train]  \n",
       "3  group of children standing in front of a schoo...  [dev_seen, dev_unseen]  \n",
       "4       woman with a black top and a blue background                 [train]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using caption generating model- BLIP, and using the provided jsonl files, the following table has been created\n",
    "#it contains caption of image, description of the image generated by BLIP\n",
    "caption_description_df= pd.read_pickle(\"../commons/caption_description.pkl\")\n",
    "caption_description_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>sets</th>\n",
       "      <th>preprocessed text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>hate count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>berserk 2016 is a good adaptation you're kidd...</td>\n",
       "      <td>71094.png</td>\n",
       "      <td>[test_unseen]</td>\n",
       "      <td>berserk good adaptation you kid right</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my life goal? make somebody this fucking trig...</td>\n",
       "      <td>91724.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>life goal make somebody this fucking trigger</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" i don't wanna, just get it, get it, get it, ...</td>\n",
       "      <td>64280.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>not wanna get get get get that shit hard chanc...</td>\n",
       "      <td>-0.163889</td>\n",
       "      <td>0.480556</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"1st day of 4th grade sandy hook elementary sc...</td>\n",
       "      <td>67082.png</td>\n",
       "      <td>[dev_seen, dev_unseen]</td>\n",
       "      <td>day grade sandy hook elementary school</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"a blow job a day will keep his side chicks aw...</td>\n",
       "      <td>46380.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>blow job day keep his side chick away -sasha grey</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text       file  \\\n",
       "0      0   berserk 2016 is a good adaptation you're kidd...  71094.png   \n",
       "1      0   my life goal? make somebody this fucking trig...  91724.png   \n",
       "2      0  \" i don't wanna, just get it, get it, get it, ...  64280.png   \n",
       "3      0  \"1st day of 4th grade sandy hook elementary sc...  67082.png   \n",
       "4      0  \"a blow job a day will keep his side chicks aw...  46380.png   \n",
       "\n",
       "                     sets                                  preprocessed text  \\\n",
       "0           [test_unseen]              berserk good adaptation you kid right   \n",
       "1                 [train]       life goal make somebody this fucking trigger   \n",
       "2                 [train]  not wanna get get get get that shit hard chanc...   \n",
       "3  [dev_seen, dev_unseen]             day grade sandy hook elementary school   \n",
       "4                 [train]  blow job day keep his side chick away -sasha grey   \n",
       "\n",
       "   polarity  subjectivity  hate count  \n",
       "0  0.492857      0.567857       0.000  \n",
       "1 -0.600000      0.800000       0.125  \n",
       "2 -0.163889      0.480556       0.080  \n",
       "3  0.300000      0.900000       0.000  \n",
       "4 -0.050000      0.100000       0.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Uncomment if necessary\n",
    "\n",
    "# caption_df= caption_description_df[[\"label\", \"text\", \"file\", \"sets\"]]\n",
    "# rets, polarity, subjectivity, hate_count= preprocessing(caption_df[\"text\"])\n",
    "# caption_df[\"preprocessed text\"]=rets\n",
    "# caption_df[\"polarity\"]=polarity\n",
    "# caption_df[\"subjectivity\"]=subjectivity\n",
    "# caption_df[\"hate count\"]= hate_count\n",
    "# pd.to_pickle(caption_df, \"../commons/caption_preprocessed.pkl\")\n",
    "\n",
    "caption_df= pd.read_pickle(\"../commons/caption_preprocessed.pkl\")\n",
    "caption_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= caption_df[[\"train\" in val for val in caption_df[\"sets\"]]]\n",
    "dev_seen_df= caption_df[[\"dev_seen\" in val for val in caption_df[\"sets\"]]]\n",
    "test_seen_df= caption_df[[\"test_seen\" in val for val in caption_df[\"sets\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vectorizer Object\n",
    "def vectorizer(dataframe, column, vocab=None):\n",
    "    document= list(dataframe[column])\n",
    "    if(vocab==None):\n",
    "        vectorizer = CountVectorizer()\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "    vectorizer.fit(document)\n",
    "    vocab= vectorizer.vocabulary_\n",
    "    vector = vectorizer.transform(document)\n",
    "    return vector, vocab\n",
    "\n",
    "def make_numpy_matrix(vector, dataframe):\n",
    "    vectorised= vector.toarray()\n",
    "    tempdf=[]\n",
    "    for i in range(len(dataframe)):\n",
    "        temp= np.append(vectorised[i], [dataframe.iloc[i][\"polarity\"], dataframe.iloc[i][\"subjectivity\"], dataframe.iloc[i][\"hate count\"], \n",
    "                                        dataframe.iloc[i][\"label\"]])\n",
    "        tempdf.append(temp)\n",
    "    tempdf= np.array(tempdf)\n",
    "    return tempdf[:,:-1], tempdf[:,-1]\n",
    "\n",
    "def print_metrics(y_pred2, y_dev):\n",
    "    print(f\"accuracy: {metrics.accuracy_score(y_pred2, y_dev)}\")\n",
    "    print(f\"f1 score: {metrics.f1_score(y_pred2, y_dev)}\")\n",
    "    print(f\"AUROC: {metrics.roc_auc_score(y_pred2, y_dev)}\")\n",
    "    print(f\"Recall: {metrics.recall_score(y_pred2, y_dev)}\") \n",
    "    print(f\"Precision: {metrics.precision_score(y_pred2, y_dev)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec, voc = vectorizer(train_df, \"text\")\n",
    "vec_dev, _ = vectorizer(dev_seen_df, \"text\", voc)\n",
    "vec_test, _ = vectorizer(test_seen_df, \"text\", voc)\n",
    "\n",
    "X_train, y_train = make_numpy_matrix(vec, train_df)\n",
    "X_test, y_test = make_numpy_matrix(vec_test, test_seen_df)\n",
    "X_dev, y_dev = make_numpy_matrix(vec_dev, dev_seen_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data below is on the seen test set alone:\n",
    "\n",
    "| Model               | accuracy_score | f1_score | roc_auc_score | recall_score | precision_score |\n",
    "|---------------------|----------------|----------|---------------|--------------|-----------------|\n",
    "| SVM rbf             | 0.549          | 0.259    | 0.598         | 0.664        | 0.161           |\n",
    "| SVM linear          | 0.560          | 0.382    | 0.579         | 0.613        | 0.278           |\n",
    "| SVM poly deg 5      | 0.513          | 0.072    | 0.527         | 0.543        | 0.039           |\n",
    "| Random Forest n=1000| 0.557          | 0.287    | 0.609         | 0.679        | 0.182           |\n",
    "| Complement Naive Bayes |       0.582 | 0.517    |  0.585        | 0.596        |            0.457|\n",
    "\n",
    "\n",
    "No matter if we choose less of features or change hyperparameters, it is still seen that model metrics mainly precision, does not improve, indicating something is not correct with underlying data itself. To be noted though, that we have tried out SVM (and Random Forests) so far, which tends to perform poorly on imbalanced datasets. \n",
    "We also observe, that Complement Naive Bayes tends to give the most balanced results in terms of precision and recall, both being close to 0.5 because complement Naive Bayes is designed to deal with imbalanced datasets. This might be an indication that other models were performing poorly because the imbalance in the dataset- 35% being offensive. We shall now try to perform oversampling and undersampling to see if performance can be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.582\n",
      "f1 score: 0.5173210161662818\n",
      "AUROC: 0.5847313147845062\n",
      "Recall: 0.5957446808510638\n",
      "Precision: 0.45714285714285713\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.56\n",
      "f1 score: 0.5175438596491229\n",
      "AUROC: 0.5606471661816209\n",
      "Recall: 0.5645933014354066\n",
      "Precision: 0.4777327935222672\n"
     ]
    }
   ],
   "source": [
    "#Note that here we only take the words into account because by definiton, naive bayes models are designed to consider word collections. \n",
    "# Having additional features would not logically make sense. \n",
    "mnb = ComplementNB()\n",
    "mnb.fit(X_train[:,:-3], y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= mnb.predict(X_test[:,:-3])\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= mnb.predict(X_dev[:,:-3])\n",
    "print_metrics(y_pred2, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.578\n",
      "f1 score: 0.44327176781002636\n",
      "AUROC: 0.5934874806296387\n",
      "Recall: 0.6268656716417911\n",
      "Precision: 0.34285714285714286\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.564\n",
      "f1 score: 0.4381443298969072\n",
      "AUROC: 0.5757916987692367\n",
      "Recall: 0.6028368794326241\n",
      "Precision: 0.3441295546558704\n"
     ]
    }
   ],
   "source": [
    "#We see a very low value of precision in an unbalanced dataset\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train[:,:-3], y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= mnb.predict(X_test[:,:-3])\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= mnb.predict(X_dev[:,:-3])\n",
    "print_metrics(y_pred2, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells are commented because they take around 10 minutes to run each, uncomment if necessary, but the findings have been reported above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier = SVC()\n",
    "# svm_classifier.fit(X_train, y_train)\n",
    "# y_pred= svm_classifier.predict(X_test)\n",
    "# print_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train, y_train)\n",
    "# importances = rf.feature_importances_\n",
    "# importances\n",
    "# y_pred= rf.predict(X_test)\n",
    "# print_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfr=2000\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# selected_features = X_train[:, indices[:selfr]]\n",
    "# rf2 = RandomForestClassifier(n_estimators=1000)\n",
    "# rf2.fit(selected_features, y_train)\n",
    "# temp2= X_test[:, indices[:selfr]]\n",
    "# y_pred2= rf2.predict(temp2)\n",
    "# print_metrics(y_pred2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now undersample the majority class, ie the non hateful memes, and then check our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = train_df[train_df['label'] == 0]\n",
    "class_1_data = train_df[train_df['label'] == 1]\n",
    "undersampled_class_0 = class_0_data.sample(n=3019, random_state=42) #there are 3019 label 1 rows\n",
    "balanced_df = pd.concat([undersampled_class_0, class_1_data])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2, voc2 = vectorizer(balanced_df, \"text\")\n",
    "vec_dev2, _ = vectorizer(dev_seen_df, \"text\", voc2)\n",
    "vec_test2, _ = vectorizer(test_seen_df, \"text\", voc2)\n",
    "\n",
    "X_train2, y_train2 = make_numpy_matrix(vec2, balanced_df)\n",
    "X_test2, y_test2 = make_numpy_matrix(vec_test2, test_seen_df)\n",
    "X_dev2, y_dev2 = make_numpy_matrix(vec_dev2, dev_seen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.58\n",
      "f1 score: 0.573170731707317\n",
      "AUROC: 0.5798915043766302\n",
      "Recall: 0.5708502024291497\n",
      "Precision: 0.5755102040816327\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.584\n",
      "f1 score: 0.5953307392996109\n",
      "AUROC: 0.5848001157351593\n",
      "Recall: 0.5730337078651685\n",
      "Precision: 0.6194331983805668\n"
     ]
    }
   ],
   "source": [
    "#We dont see a very significant difference from before because Complement Naive Bayes has been designed for imbalanced datasets\n",
    "mnb2 = ComplementNB()\n",
    "mnb2.fit(X_train2[:,:-3], y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= mnb2.predict(X_test2[:,:-3])\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t= mnb2.predict(X_dev2[:,:-3])\n",
    "print_metrics(y_pred_t, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.58\n",
      "f1 score: 0.573170731707317\n",
      "AUROC: 0.5798915043766302\n",
      "Recall: 0.5708502024291497\n",
      "Precision: 0.5755102040816327\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.584\n",
      "f1 score: 0.5953307392996109\n",
      "AUROC: 0.5848001157351593\n",
      "Recall: 0.5730337078651685\n",
      "Precision: 0.6194331983805668\n"
     ]
    }
   ],
   "source": [
    "#As opposed to earlier though, we see a great improvement in precision\n",
    "mnb2 = MultinomialNB()\n",
    "mnb2.fit(X_train2[:,:-3], y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= mnb2.predict(X_test2[:,:-3])\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t= mnb2.predict(X_dev2[:,:-3])\n",
    "print_metrics(y_pred_t, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.58\n",
      "f1 score: 0.44297082228116713\n",
      "AUROC: 0.5968585309617919\n",
      "Recall: 0.6325757575757576\n",
      "Precision: 0.3408163265306122\n"
     ]
    }
   ],
   "source": [
    "#Although the precision is still low, we see a large improvement from previous case. We however know that SVMs do not perfrom\n",
    "# well on larger datasets, and our feature space is particularly large\n",
    "svm_classifier2 = SVC()\n",
    "svm_classifier2.fit(X_train2, y_train2)\n",
    "y_pred2= svm_classifier2.predict(X_test2)\n",
    "print_metrics(y_pred2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.594\n",
      "f1 score: 0.5072815533980582\n",
      "AUROC: 0.601913290536045\n",
      "Recall: 0.625748502994012\n",
      "Precision: 0.42653061224489797\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.586\n",
      "f1 score: 0.5174825174825175\n",
      "AUROC: 0.5911085769576336\n",
      "Recall: 0.6098901098901099\n",
      "Precision: 0.4493927125506073\n"
     ]
    }
   ],
   "source": [
    "lrcf = LogisticRegression()\n",
    "lrcf.fit(X_train2, y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= lrcf.predict(X_test2)\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= lrcf.predict(X_dev2)\n",
    "print_metrics(y_pred2, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.571\n",
      "f1 score: 0.362555720653789\n",
      "AUROC: 0.6081191350469195\n",
      "Recall: 0.6666666666666666\n",
      "Precision: 0.24897959183673468\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.544\n",
      "f1 score: 0.3522727272727273\n",
      "AUROC: 0.5610608800482217\n",
      "Recall: 0.5904761904761905\n",
      "Precision: 0.25101214574898784\n"
     ]
    }
   ],
   "source": [
    "#As we can see, when we use the original imbalanced dataset is giving poor results\n",
    "lrcf = LogisticRegression()\n",
    "lrcf.fit(X_train, y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= lrcf.predict(X_test)\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred= lrcf.predict(X_dev)\n",
    "print_metrics(y_pred, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also try oversampling the minority class to see if there are any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = train_df[train_df['label'] == 0]\n",
    "class_1_data = train_df[train_df['label'] == 1]\n",
    "difference= len(class_0_data)-len(class_1_data)\n",
    "oversampled_class_1 = class_1_data.sample(n=difference, random_state=42) \n",
    "balanced_df2 = pd.concat([class_0_data, class_1_data, oversampled_class_1])\n",
    "balanced_df2 = balanced_df2.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3, voc3 = vectorizer(balanced_df2, \"text\")\n",
    "vec_dev3, _ = vectorizer(dev_seen_df, \"text\", voc3)\n",
    "vec_test3, _ = vectorizer(test_seen_df, \"text\", voc3)\n",
    "\n",
    "X_train3, y_train3 = make_numpy_matrix(vec3, balanced_df2)\n",
    "X_test3, y_test3 = make_numpy_matrix(vec_test3, test_seen_df)\n",
    "X_dev3, y_dev3 = make_numpy_matrix(vec_dev3, dev_seen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.587\n",
      "f1 score: 0.5525460455037919\n",
      "AUROC: 0.5872262342624159\n",
      "Recall: 0.5889145496535797\n",
      "Precision: 0.5204081632653061\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.57\n",
      "f1 score: 0.5548654244306418\n",
      "AUROC: 0.5698831535695943\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.5425101214574899\n"
     ]
    }
   ],
   "source": [
    "#There is a slight decrease in performance, but that is expected, because we are repeating information, bringing in redundancy\n",
    "#Yet, even in this case, all the metrics are above 0.5 indicating some information or pattern has been learnt\n",
    "mnb3 = MultinomialNB()\n",
    "mnb3.fit(X_train3[:,:-3], y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= mnb3.predict(X_test3[:,:-3])\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= mnb3.predict(X_dev3[:,:-3])\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.587\n",
      "f1 score: 0.5525460455037919\n",
      "AUROC: 0.5872262342624159\n",
      "Recall: 0.5889145496535797\n",
      "Precision: 0.5204081632653061\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.57\n",
      "f1 score: 0.5548654244306418\n",
      "AUROC: 0.5698831535695943\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.5425101214574899\n"
     ]
    }
   ],
   "source": [
    "mnb3 = ComplementNB()\n",
    "mnb3.fit(X_train3[:,:-3], y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= mnb3.predict(X_test3[:,:-3])\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= mnb3.predict(X_dev3[:,:-3])\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We clearly see a drop in performance here as well, reinforcing our idea that undersampling is more suited in our problem\n",
    "clf_lr2 = LogisticRegression()\n",
    "clf_lr2.fit(X_train3, y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= clf_lr2.predict(X_test3)\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= clf_lr2.predict(X_dev3)\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original\n",
    "\n",
    "| Model | Dataset | Accuracy | F1 Score | AUROC | Recall | Precision |\n",
    "|-------|---------|----------|----------|-------|--------|-----------|\n",
    "| Complement Naive Bayes | Test Seen | 0.582 | 0.517 | 0.585 | 0.596 | 0.457 |\n",
    "|                        | Dev Seen  | 0.560 | 0.518 | 0.561 | 0.565 | 0.478 |\n",
    "| Multinomial Naive Bayes | Test Seen | 0.578 | 0.443 | 0.593 | 0.627 | 0.343 |\n",
    "|                          | Dev Seen  | 0.564 | 0.438 | 0.576 | 0.603 | 0.344 |\n",
    "| Logistic Regression     | Test Seen | 0.571 | 0.363 | 0.608 | 0.667 | 0.249 |\n",
    "|                          | Dev Seen  | 0.544 | 0.352 | 0.561 | 0.590 | 0.251 |\n",
    "\n",
    "\n",
    "\n",
    "### Undersampling \n",
    "\n",
    "| Model | Dataset | Accuracy | F1 Score | AUROC | Recall | Precision |\n",
    "|-------|---------|----------|----------|-------|--------|-----------|\n",
    "| Complement Naive Bayes | Test Seen | 0.58 | 0.573 | 0.580 | 0.571 | 0.576 |\n",
    "|                        | Dev Seen  | 0.584 | 0.595 | 0.585 | 0.573 | 0.619 |\n",
    "| Multinomial Naive Bayes | Test Seen | 0.58 | 0.573 | 0.580 | 0.571 | 0.576 |\n",
    "|                          | Dev Seen  | 0.584 | 0.595 | 0.585 | 0.573 | 0.619 |\n",
    "| Logistic Regression     | Test Seen | 0.594 | 0.507 | 0.602 | 0.626 | 0.427 |\n",
    "|                        | Dev Seen  | 0.586 | 0.517 | 0.591 | 0.610 | 0.449 |\n",
    "\n",
    "\n",
    "### Oversampling\n",
    "\n",
    "| Model | Dataset | Accuracy | F1 Score | AUROC | Recall | Precision |\n",
    "|-------|---------|----------|----------|-------|--------|-----------|\n",
    "| Complement Naive Bayes | Test Seen | 0.587 | 0.553 | 0.587 | 0.589 | 0.520 |\n",
    "|                        | Dev Seen  | 0.570 | 0.555 | 0.570 | 0.568 | 0.543 |\n",
    "| Multinomial Naive Bayes | Test Seen | 0.587 | 0.553 | 0.587 | 0.589 | 0.520 |\n",
    "|                          | Dev Seen  | 0.570 | 0.555 | 0.570 | 0.568 | 0.543 |\n",
    "| Logistic Regression     | Test Seen | 0.591 | 0.475 | 0.606 | 0.640 | 0.378 |\n",
    "|                          | Dev Seen  | 0.564 | 0.468 | 0.570 | 0.589 | 0.389 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "1. We see that SVMs do not work as a preliminary model for textual analysis because SVMs perform poorly on large datasets, and additionally, they also do not perform well on imbalanced datasets. We see an improvement in their performance (albeit slightly) with undersampling, yet they do not perfrom sufficiently well so their use was subsequently eliminated\n",
    "\n",
    "2. We see that there is a significant improvement in the metrics (primarily precision) when we undersample and oversample the dataset as compared to the original dataset. However, we see that undersampling gives better results as opposed to oversampling. This might be because we are repeating the data which results in a repeat of the vocabulary, and as a result, some words that are common to both offensive and non offensive become more likely to be classified as offensive resulting in a greater number of false positives. It is important to note that we are repeating a lot of rows while oversampling due to a huge imbalance in dataset. \n",
    "\n",
    "3. Other methods of dataset imbalance removal such as SMOTE are known to not work well for textual data and since our data is primarily from NLP tasks, we do not use SMOTE\n",
    "\n",
    "4. It is also important to note that the values for undersampling, although better, are still very low compared to human performance as well as baseline and SOTA models. This is because we are solely using textual data for our models, and this completely eliminates the information from the images. The hateful meme classification task is intended to be multimodal in nature and our model does not take into account the image modality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
