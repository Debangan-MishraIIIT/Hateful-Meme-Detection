{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "nlp.add_pipe('spacytextblob')\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'i', 'me', 'my', 'myself', 'it', 'its', 'itself', 'what', 'which', 'who', 'whom', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n",
    "    'own', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "\n",
    "hate_speech_words = [\n",
    "    'nigger', 'nigga', 'coon', 'spic', 'chink', 'kike', 'gook', 'wop', 'towelhead', 'slope',\n",
    "    'kafir', 'infidel', 'heathen', 'idolater', 'heretic', 'blasphemer',\n",
    "    'bitch', 'slut', 'whore', 'cunt', 'feminazi', 'tranny', 'dyke', 'faggot', 'fag', 'pussy', 'mansplainer',\n",
    "    'faggot', 'fag', 'dyke', 'homo', 'queer', 'sissy', 'tranny', 'butch',\n",
    "    'tranny', 'shemale', 'it', 'transvestite', 'gender bender', 'freak',\n",
    "    'retard', 'spaz', 'cripple', 'invalid', 'lame', 'imbecile', 'moron', 'idiot',\n",
    "    'fucktard', 'asshat', 'retard', 'douchebag', 'assclown', 'twat', 'twatwaffle', 'dickwad'\n",
    "]\n",
    "\n",
    "swear_words = [\n",
    "    'fuck', 'fucking', 'fucked', 'fuckwit', 'fucktard', 'fuckface', 'fuckhead', 'motherfucker', 'shit', 'shitty', 'shite', 'shitting', \n",
    "    'shithead', 'shitstorm', 'ass', 'asshole', 'asshat', 'asswipe', 'dumbass', 'bitch', 'bitchy', 'son of a bitch',\n",
    "    'bastard', 'damn', 'damned', 'dick', 'dickhead', 'dickwad', 'pussy', 'pussies', 'cock', 'cocksucker', 'cockhead',\n",
    "    'cunt', 'cunty', 'cuntface', 'twat', 'twatwaffle', 'wanker', 'wank', 'wankstain', 'bollocks', 'douchebag', 'douche', 'douchecanoe',\n",
    "    'arse', 'arsehole', 'arsehat', 'arsewipe', 'shitfuck', 'fuckshit', 'clusterfuck', 'fucknugget', 'fuckery',\n",
    "    'cockwomble', 'cockgobbler', 'cockjockey', 'dickcheese', 'dicknose', 'shitlord', 'assclown', 'assjacket'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "      rets=[]\n",
    "      polarity=[]\n",
    "      subjectivity=[]\n",
    "      hate_count=[]\n",
    "      for sent in sentences:\n",
    "            sent = sent.replace(\"n't\", \" not\")\n",
    "            sent = sent.lower().strip()\n",
    "            temp_arr= sent.split(\" \")\n",
    "            count=0\n",
    "            for word in temp_arr:\n",
    "                  if(word in hate_speech_words or word in swear_words):\n",
    "                        count+=1\n",
    "            hate_count.append(count/len(temp_arr))\n",
    "            doc = nlp(sent)\n",
    "            arr = [token.lemma_ for token in doc if (token.lemma_.lower() not in STOP_WORDS) and not token.is_punct and not token.like_num \n",
    "                  and not token.is_currency and not token.is_digit]\n",
    "            arr= \" \".join(arr)\n",
    "            rets.append(arr)\n",
    "            polarity.append(doc._.blob.polarity)\n",
    "            subjectivity.append(doc._.blob.subjectivity)\n",
    "      return rets, polarity, subjectivity, hate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>description</th>\n",
       "      <th>sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>berserk 2016 is a good adaptation you're kidd...</td>\n",
       "      <td>71094.png</td>\n",
       "      <td>woman with a monkey mask and a fake monkey</td>\n",
       "      <td>[test_unseen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my life goal? make somebody this fucking trig...</td>\n",
       "      <td>91724.png</td>\n",
       "      <td>woman holding a cigarette in her hand</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" i don't wanna, just get it, get it, get it, ...</td>\n",
       "      <td>64280.png</td>\n",
       "      <td>man wearing a hat and a tie</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"1st day of 4th grade sandy hook elementary sc...</td>\n",
       "      <td>67082.png</td>\n",
       "      <td>group of children standing in front of a schoo...</td>\n",
       "      <td>[dev_seen, dev_unseen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"a blow job a day will keep his side chicks aw...</td>\n",
       "      <td>46380.png</td>\n",
       "      <td>woman with a black top and a blue background</td>\n",
       "      <td>[train]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text       file  \\\n",
       "0      0   berserk 2016 is a good adaptation you're kidd...  71094.png   \n",
       "1      0   my life goal? make somebody this fucking trig...  91724.png   \n",
       "2      0  \" i don't wanna, just get it, get it, get it, ...  64280.png   \n",
       "3      0  \"1st day of 4th grade sandy hook elementary sc...  67082.png   \n",
       "4      0  \"a blow job a day will keep his side chicks aw...  46380.png   \n",
       "\n",
       "                                         description                    sets  \n",
       "0         woman with a monkey mask and a fake monkey           [test_unseen]  \n",
       "1              woman holding a cigarette in her hand                 [train]  \n",
       "2                        man wearing a hat and a tie                 [train]  \n",
       "3  group of children standing in front of a schoo...  [dev_seen, dev_unseen]  \n",
       "4       woman with a black top and a blue background                 [train]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using caption generating model- BLIP, and using the provided jsonl files, the following table has been created\n",
    "#it contains caption of image, description of the image generated by BLIP\n",
    "caption_description_df= pd.read_pickle(\"../commons/caption_description.pkl\")\n",
    "caption_description_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>sets</th>\n",
       "      <th>preprocessed text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>hate count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>berserk 2016 is a good adaptation you're kidd...</td>\n",
       "      <td>71094.png</td>\n",
       "      <td>[test_unseen]</td>\n",
       "      <td>berserk good adaptation you kid right</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my life goal? make somebody this fucking trig...</td>\n",
       "      <td>91724.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>life goal make somebody this fucking trigger</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" i don't wanna, just get it, get it, get it, ...</td>\n",
       "      <td>64280.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>not wanna get get get get that shit hard chanc...</td>\n",
       "      <td>-0.163889</td>\n",
       "      <td>0.480556</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"1st day of 4th grade sandy hook elementary sc...</td>\n",
       "      <td>67082.png</td>\n",
       "      <td>[dev_seen, dev_unseen]</td>\n",
       "      <td>day grade sandy hook elementary school</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"a blow job a day will keep his side chicks aw...</td>\n",
       "      <td>46380.png</td>\n",
       "      <td>[train]</td>\n",
       "      <td>blow job day keep his side chick away -sasha grey</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text       file  \\\n",
       "0      0   berserk 2016 is a good adaptation you're kidd...  71094.png   \n",
       "1      0   my life goal? make somebody this fucking trig...  91724.png   \n",
       "2      0  \" i don't wanna, just get it, get it, get it, ...  64280.png   \n",
       "3      0  \"1st day of 4th grade sandy hook elementary sc...  67082.png   \n",
       "4      0  \"a blow job a day will keep his side chicks aw...  46380.png   \n",
       "\n",
       "                     sets                                  preprocessed text  \\\n",
       "0           [test_unseen]              berserk good adaptation you kid right   \n",
       "1                 [train]       life goal make somebody this fucking trigger   \n",
       "2                 [train]  not wanna get get get get that shit hard chanc...   \n",
       "3  [dev_seen, dev_unseen]             day grade sandy hook elementary school   \n",
       "4                 [train]  blow job day keep his side chick away -sasha grey   \n",
       "\n",
       "   polarity  subjectivity  hate count  \n",
       "0  0.492857      0.567857       0.000  \n",
       "1 -0.600000      0.800000       0.125  \n",
       "2 -0.163889      0.480556       0.080  \n",
       "3  0.300000      0.900000       0.000  \n",
       "4 -0.050000      0.100000       0.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Uncomment if necessary\n",
    "\n",
    "# caption_df= caption_description_df[[\"label\", \"text\", \"file\", \"sets\"]]\n",
    "# rets, polarity, subjectivity, hate_count= preprocessing(caption_df[\"text\"])\n",
    "# caption_df[\"preprocessed text\"]=rets\n",
    "# caption_df[\"polarity\"]=polarity\n",
    "# caption_df[\"subjectivity\"]=subjectivity\n",
    "# caption_df[\"hate count\"]= hate_count\n",
    "# pd.to_pickle(caption_df, \"../commons/caption_preprocessed.pkl\")\n",
    "\n",
    "caption_df= pd.read_pickle(\"../commons/caption_preprocessed.pkl\")\n",
    "caption_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= caption_df[[\"train\" in val for val in caption_df[\"sets\"]]]\n",
    "dev_seen_df= caption_df[[\"dev_seen\" in val for val in caption_df[\"sets\"]]]\n",
    "test_seen_df= caption_df[[\"test_seen\" in val for val in caption_df[\"sets\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vectorizer Object\n",
    "def vectorizer(dataframe, column, vocab=None):\n",
    "    document= list(dataframe[column])\n",
    "    if(vocab==None):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "    vectorizer.fit(document)\n",
    "    vocab= vectorizer.vocabulary_\n",
    "    vector = vectorizer.transform(document)\n",
    "    return vector, vocab\n",
    "\n",
    "def make_numpy_matrix(vector, dataframe):\n",
    "    vectorised= vector.toarray()\n",
    "    tempdf=[]\n",
    "    for i in range(len(dataframe)):\n",
    "        temp= np.append(vectorised[i], [dataframe.iloc[i][\"polarity\"], dataframe.iloc[i][\"subjectivity\"], dataframe.iloc[i][\"hate count\"], \n",
    "                                        dataframe.iloc[i][\"label\"]])\n",
    "        tempdf.append(temp)\n",
    "    tempdf= np.array(tempdf)\n",
    "    return tempdf[:,:-1], tempdf[:,-1]\n",
    "\n",
    "def print_metrics(y_pred2, y_dev):\n",
    "    print(f\"accuracy: {metrics.accuracy_score(y_pred2, y_dev)}\")\n",
    "    print(f\"f1 score: {metrics.f1_score(y_pred2, y_dev)}\")\n",
    "    print(f\"AUROC: {metrics.roc_auc_score(y_pred2, y_dev)}\")\n",
    "    print(f\"Recall: {metrics.recall_score(y_pred2, y_dev)}\") \n",
    "    print(f\"Precision: {metrics.precision_score(y_pred2, y_dev)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec, voc = vectorizer(train_df, \"text\")\n",
    "vec_dev, _ = vectorizer(dev_seen_df, \"text\", voc)\n",
    "vec_test, _ = vectorizer(test_seen_df, \"text\", voc)\n",
    "\n",
    "X_train, y_train = make_numpy_matrix(vec, train_df)\n",
    "X_test, y_test = make_numpy_matrix(vec_test, test_seen_df)\n",
    "X_dev, y_dev = make_numpy_matrix(vec_dev, dev_seen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.566\n",
      "f1 score: 0.4166666666666667\n",
      "AUROC: 0.5805872791370248\n",
      "Recall: 0.610236220472441\n",
      "Precision: 0.3163265306122449\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.546\n",
      "f1 score: 0.39466666666666667\n",
      "AUROC: 0.5565356182795699\n",
      "Recall: 0.578125\n",
      "Precision: 0.29959514170040485\n"
     ]
    }
   ],
   "source": [
    "#Note that here we only take the words into account because by definiton, naive bayes models are designed to consider word collections. \n",
    "# Having additional features would not logically make sense. \n",
    "mnb = ComplementNB()\n",
    "mnb.fit(X_train[:,:-3], y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= mnb.predict(X_test[:,:-3])\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= mnb.predict(X_dev[:,:-3])\n",
    "print_metrics(y_pred2, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.53\n",
      "f1 score: 0.12962962962962962\n",
      "AUROC: 0.6105263157894737\n",
      "Recall: 0.7\n",
      "Precision: 0.07142857142857142\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.52\n",
      "f1 score: 0.1366906474820144\n",
      "AUROC: 0.5633812504298782\n",
      "Recall: 0.6129032258064516\n",
      "Precision: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "#We see a very low value of precision in an unbalanced dataset\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train[:,:-3], y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= mnb.predict(X_test[:,:-3])\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= mnb.predict(X_dev[:,:-3])\n",
    "print_metrics(y_pred2, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells are commented because they take around 10 minutes to run each, uncomment if necessary, but the findings have been reported above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier = SVC()\n",
    "# svm_classifier.fit(X_train, y_train)\n",
    "# y_pred= svm_classifier.predict(X_test)\n",
    "# print_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train, y_train)\n",
    "# importances = rf.feature_importances_\n",
    "# importances\n",
    "# y_pred= rf.predict(X_test)\n",
    "# print_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfr=2000\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# selected_features = X_train[:, indices[:selfr]]\n",
    "# rf2 = RandomForestClassifier(n_estimators=1000)\n",
    "# rf2.fit(selected_features, y_train)\n",
    "# temp2= X_test[:, indices[:selfr]]\n",
    "# y_pred2= rf2.predict(temp2)\n",
    "# print_metrics(y_pred2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now undersample the majority class, ie the non hateful memes, and then check our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = train_df[train_df['label'] == 0]\n",
    "class_1_data = train_df[train_df['label'] == 1]\n",
    "undersampled_class_0 = class_0_data.sample(n=3019, random_state=42) #there are 3019 label 1 rows\n",
    "balanced_df = pd.concat([undersampled_class_0, class_1_data])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2, voc2 = vectorizer(balanced_df, \"text\")\n",
    "vec_dev2, _ = vectorizer(dev_seen_df, \"text\", voc2)\n",
    "vec_test2, _ = vectorizer(test_seen_df, \"text\", voc2)\n",
    "\n",
    "X_train2, y_train2 = make_numpy_matrix(vec2, balanced_df)\n",
    "X_test2, y_test2 = make_numpy_matrix(vec_test2, test_seen_df)\n",
    "X_dev2, y_dev2 = make_numpy_matrix(vec_dev2, dev_seen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.593\n",
      "f1 score: 0.5901309164149043\n",
      "AUROC: 0.5930633502806101\n",
      "Recall: 0.5825049701789264\n",
      "Precision: 0.5979591836734693\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.592\n",
      "f1 score: 0.6091954022988506\n",
      "AUROC: 0.5935353535353535\n",
      "Recall: 0.5781818181818181\n",
      "Precision: 0.6437246963562753\n"
     ]
    }
   ],
   "source": [
    "#We dont see a very significant difference from before because Complement Naive Bayes has been designed for imbalanced datasets\n",
    "mnb2 = ComplementNB()\n",
    "mnb2.fit(X_train2[:,:-3], y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= mnb2.predict(X_test2[:,:-3])\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t= mnb2.predict(X_dev2[:,:-3])\n",
    "print_metrics(y_pred_t, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.593\n",
      "f1 score: 0.5901309164149043\n",
      "AUROC: 0.5930633502806101\n",
      "Recall: 0.5825049701789264\n",
      "Precision: 0.5979591836734693\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.592\n",
      "f1 score: 0.6091954022988506\n",
      "AUROC: 0.5935353535353535\n",
      "Recall: 0.5781818181818181\n",
      "Precision: 0.6437246963562753\n"
     ]
    }
   ],
   "source": [
    "#As opposed to earlier though, we see a great improvement in precision\n",
    "mnb2 = MultinomialNB()\n",
    "mnb2.fit(X_train2[:,:-3], y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= mnb2.predict(X_test2[:,:-3])\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t= mnb2.predict(X_dev2[:,:-3])\n",
    "print_metrics(y_pred_t, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.59\n",
      "f1 score: 0.5308924485125858\n",
      "AUROC: 0.5926677489177489\n",
      "Recall: 0.6041666666666666\n",
      "Precision: 0.47346938775510206\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.592\n",
      "f1 score: 0.5486725663716814\n",
      "AUROC: 0.5939644481190575\n",
      "Recall: 0.6048780487804878\n",
      "Precision: 0.5020242914979757\n"
     ]
    }
   ],
   "source": [
    "lrcf = LogisticRegression()\n",
    "lrcf.fit(X_train2, y_train2)\n",
    "print(\"Test Seen\")\n",
    "y_pred2= lrcf.predict(X_test2)\n",
    "print_metrics(y_pred2, y_test2)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred2= lrcf.predict(X_dev2)\n",
    "print_metrics(y_pred2, y_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.561\n",
      "f1 score: 0.30866141732283464\n",
      "AUROC: 0.6086912684008873\n",
      "Recall: 0.6758620689655173\n",
      "Precision: 0.2\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.538\n",
      "f1 score: 0.28923076923076924\n",
      "AUROC: 0.5643152266375016\n",
      "Recall: 0.6025641025641025\n",
      "Precision: 0.1902834008097166\n"
     ]
    }
   ],
   "source": [
    "#As we can see, when we use the original imbalanced dataset is giving poor results\n",
    "lrcf = LogisticRegression()\n",
    "lrcf.fit(X_train, y_train)\n",
    "print(\"Test Seen\")\n",
    "y_pred= lrcf.predict(X_test)\n",
    "print_metrics(y_pred, y_test)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred= lrcf.predict(X_dev)\n",
    "print_metrics(y_pred, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also try oversampling the minority class to see if there are any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = train_df[train_df['label'] == 0]\n",
    "class_1_data = train_df[train_df['label'] == 1]\n",
    "difference= len(class_0_data)-len(class_1_data)\n",
    "oversampled_class_1 = class_1_data.sample(n=difference, random_state=42) \n",
    "balanced_df2 = pd.concat([class_0_data, class_1_data, oversampled_class_1])\n",
    "balanced_df2 = balanced_df2.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3, voc3 = vectorizer(balanced_df2, \"text\")\n",
    "vec_dev3, _ = vectorizer(dev_seen_df, \"text\", voc3)\n",
    "vec_test3, _ = vectorizer(test_seen_df, \"text\", voc3)\n",
    "\n",
    "X_train3, y_train3 = make_numpy_matrix(vec3, balanced_df2)\n",
    "X_test3, y_test3 = make_numpy_matrix(vec_test3, test_seen_df)\n",
    "X_dev3, y_dev3 = make_numpy_matrix(vec_dev3, dev_seen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.574\n",
      "f1 score: 0.5571725571725572\n",
      "AUROC: 0.5736710323574731\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.5469387755102041\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.578\n",
      "f1 score: 0.5667351129363449\n",
      "AUROC: 0.5778846153846153\n",
      "Recall: 0.575\n",
      "Precision: 0.5587044534412956\n"
     ]
    }
   ],
   "source": [
    "#There is a slight decrease in performance, but that is expected, because we are repeating information, bringing in redundancy\n",
    "#Yet, even in this case, all the metrics are above 0.5 indicating some information or pattern has been learnt\n",
    "mnb3 = MultinomialNB()\n",
    "mnb3.fit(X_train3[:,:-3], y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= mnb3.predict(X_test3[:,:-3])\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= mnb3.predict(X_dev3[:,:-3])\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.574\n",
      "f1 score: 0.5571725571725572\n",
      "AUROC: 0.5736710323574731\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.5469387755102041\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.578\n",
      "f1 score: 0.5667351129363449\n",
      "AUROC: 0.5778846153846153\n",
      "Recall: 0.575\n",
      "Precision: 0.5587044534412956\n"
     ]
    }
   ],
   "source": [
    "mnb3 = ComplementNB()\n",
    "mnb3.fit(X_train3[:,:-3], y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= mnb3.predict(X_test3[:,:-3])\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= mnb3.predict(X_dev3[:,:-3])\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Seen\n",
      "accuracy: 0.599\n",
      "f1 score: 0.52989449003517\n",
      "AUROC: 0.6040734157617274\n",
      "Recall: 0.6225895316804407\n",
      "Precision: 0.46122448979591835\n",
      "\n",
      "Dev Seen\n",
      "accuracy: 0.592\n",
      "f1 score: 0.5405405405405406\n",
      "AUROC: 0.5949975708230721\n",
      "Recall: 0.6091370558375635\n",
      "Precision: 0.48582995951417\n"
     ]
    }
   ],
   "source": [
    "#We clearly see a drop in performance here as well, reinforcing our idea that undersampling is more suited in our problem\n",
    "clf_lr2 = LogisticRegression()\n",
    "clf_lr2.fit(X_train3, y_train3)\n",
    "print(\"Test Seen\")\n",
    "y_pred3= clf_lr2.predict(X_test3)\n",
    "print_metrics(y_pred3, y_test3)\n",
    "\n",
    "print(\"\\nDev Seen\")\n",
    "y_pred_t3= clf_lr2.predict(X_dev3)\n",
    "print_metrics(y_pred_t3, y_dev3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
